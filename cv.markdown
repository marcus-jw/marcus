---
layout: cv
title: CV
permalink: /cv/
---

## Experience

### MATS
**Research Scholar** | Berkeley, USA | June 2024--Present

Working on building a benchmark for the tendency of LLM agents to influence human preferences in generative RL environments.
[Project Repository](https://github.com/marcus-jw/Targeted-Manipulation-and-Deception-in-LLMs)

### Research grant by the Long-Term Future Fund
**Multi-Objective Reinforcement Learning from AI Feedback** | Oct 2023--May 2024

Research project investigating using multiple preference models and multi-objective RL approaches to improve safety-performance of LLMs. See publication below.
[Project Repository](https://github.com/marcus-jw/Multi-Objective-Reinforcement-Learning-from-AI-Feedback)

### AI Safety Hub Oxford
**Researching the expressivity of different RL formalisms** | Oxford, UK | Jul 2023--Oct 2023

Formally proved the expressivity relationships between various reinforcement learning formalisms, see publication below.

### AI Safety Fundamentals
**Course Facilitator** | Lund, Sweden | Aug 2023--Dec 2023

Facilitating multiple in-person groups for the AI Safety Fundamentals Alignment course.

### Axis Communications
**Software Developer in Mobile Applications** | Lund, Sweden | 2019--2022

I worked on maintaining and improving their Android mobile applications through error and latency analysis.

## Education

### The Faculty of Engineering at Lund University
**Master of Engineering (MEng)** | Lund, Sweden | 2018--2023

- Grade: 5.0/5.0
- Combined 5 year bachelor's and master's in Engineering Physics specialising in Machine Learning

## Publications

### ICLR 2024
**"On The Expressivity of Objective-Specification Formalisms in RL"**

We investigated the expressive power of 17 objective-specification formalisms in reinforcement learning (RL). We conducted a comprehensive analysis, categorizing formalisms in a Hasse diagram to reveal relative expressiveness and trade-offs with optimization ease. Our work demonstrates that no single formalism dominates, and we identify objectives uniquely expressible by specific formalisms.
[Paper](https://arxiv.org/abs/2310.11840)

### Pending
**"Multi-objective Reinforcement learning from AI Feedback"**

I investigated improving LLM alignment by decomposing human preferences into multiple principles and training a separate preference model of each. MORLAIF outperforms standard RLAIF baselines and can effectively align larger language models using smaller ones, regardless of the scalarization function used to combine preference model scores.
[Paper](https://arxiv.org/abs/2406.07295)
